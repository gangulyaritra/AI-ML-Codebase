{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MissingValues.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaFzKJlJbuPs"
      },
      "source": [
        "# **Methods for Dealing with Missing Values.**\n",
        "\n",
        "> [**Missing Data - Wikipedia**](https://en.wikipedia.org/wiki/Missing_data)\n",
        "\n",
        "In statistics, missing data, or missing values, occur when no data value gets stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. Missing data can occur because of nonresponse, i.e., no information is provided for one or more items or a whole unit (\"subject\"). These forms of missingness take different types, with different impacts on the validity of conclusions from the research.\n",
        "\n",
        "*   **Missing Completely at Random**\n",
        "*   **Missing at Random**\n",
        "*   **Missing Not at Random**\n",
        "\n",
        ">  **Missing Completely at Random:** Values in a dataset are **missing completely at random (MCAR)** if the events that lead to any particular data item being missing are independent both of observable variables and unobservable parameters of interest and occur entirely at random. When data are MCAR, the analysis performed on the data is unbiased; however, data are rarely MCAR. In the case of MCAR, the missingness of data is unrelated to any study variable. Thus, the participants with completely observed data are in effect a random sample of all the participants assigned a particular intervention. With MCAR, the random assignment of treatments is assumed to be preserved, but that is usually an unrealistically strong assumption in practice.\n",
        "\n",
        ">  **Missing at Random:** Missing at random (MAR) occurs when the missingness is not random, but where missingness can be fully accounted for by variables where there is complete information. Since MAR is an assumption that is impossible to verify statistically, we must rely on its substantive reasonableness. An example is that males are less likely to fill in a depression survey, but this has nothing to do with their level of depression after accounting for maleness. Depending on the analysis method, these data can still induce parameter bias in analyses due to the contingent emptiness of cells (male, very high depression may have zero entries). However, if the parameter gets estimated with Full Information Maximum Likelihood, MAR will provide asymptotically unbiased estimates.\n",
        "\n",
        ">  **Missing Not at Random:** Missing not at random (MNAR) is data that is neither MAR nor MCAR (i.e., the value of the variable that's missing is related to the reason it's missing). To extend the previous example, this would occur if men failed to fill in a depression survey because of their level of depression."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FMq8suIeii4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0jl08mwmZ_A"
      },
      "source": [
        "## **Techniques of Dealing with Missing Data.**\n",
        "\n",
        "Missing data reduces the representativeness of the sample and can therefore distort inferences about the population. Generally speaking, there are three main approaches to handle missing data:\n",
        "\n",
        "1.   **Imputation:** where values are filled in the place of missing data.\n",
        "\n",
        "2.   **Omission:** where samples with invalid data are discarded from further analysis.\n",
        "\n",
        "3.   **Analysis:** by directly applying methods unaffected by the missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0TMHuUBouKF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c48f6f52-096d-4b87-92e9-bc86689e2a15"
      },
      "source": [
        "# Import Library.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Dataset.\n",
        "data = pd.read_csv(\n",
        "    \"http://www.creditriskanalytics.net/uploads/1/9/5/1/19511601/hmeq.csv\"\n",
        ")\n",
        "data.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   BAD  LOAN  MORTDUE     VALUE   REASON     JOB   YOJ  DEROG  DELINQ  \\\n",
              "0    1  1100  25860.0   39025.0  HomeImp   Other  10.5    0.0     0.0   \n",
              "1    1  1300  70053.0   68400.0  HomeImp   Other   7.0    0.0     2.0   \n",
              "2    1  1500  13500.0   16700.0  HomeImp   Other   4.0    0.0     0.0   \n",
              "3    1  1500      NaN       NaN      NaN     NaN   NaN    NaN     NaN   \n",
              "4    0  1700  97800.0  112000.0  HomeImp  Office   3.0    0.0     0.0   \n",
              "\n",
              "        CLAGE  NINQ  CLNO  DEBTINC  \n",
              "0   94.366667   1.0   9.0      NaN  \n",
              "1  121.833333   0.0  14.0      NaN  \n",
              "2  149.466667   1.0  10.0      NaN  \n",
              "3         NaN   NaN   NaN      NaN  \n",
              "4   93.333333   0.0  14.0      NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ef0332e-a24e-4db0-bbef-ce039a67777c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BAD</th>\n",
              "      <th>LOAN</th>\n",
              "      <th>MORTDUE</th>\n",
              "      <th>VALUE</th>\n",
              "      <th>REASON</th>\n",
              "      <th>JOB</th>\n",
              "      <th>YOJ</th>\n",
              "      <th>DEROG</th>\n",
              "      <th>DELINQ</th>\n",
              "      <th>CLAGE</th>\n",
              "      <th>NINQ</th>\n",
              "      <th>CLNO</th>\n",
              "      <th>DEBTINC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1100</td>\n",
              "      <td>25860.0</td>\n",
              "      <td>39025.0</td>\n",
              "      <td>HomeImp</td>\n",
              "      <td>Other</td>\n",
              "      <td>10.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.366667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1300</td>\n",
              "      <td>70053.0</td>\n",
              "      <td>68400.0</td>\n",
              "      <td>HomeImp</td>\n",
              "      <td>Other</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>121.833333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1500</td>\n",
              "      <td>13500.0</td>\n",
              "      <td>16700.0</td>\n",
              "      <td>HomeImp</td>\n",
              "      <td>Other</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>149.466667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1700</td>\n",
              "      <td>97800.0</td>\n",
              "      <td>112000.0</td>\n",
              "      <td>HomeImp</td>\n",
              "      <td>Office</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>93.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ef0332e-a24e-4db0-bbef-ce039a67777c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ef0332e-a24e-4db0-bbef-ce039a67777c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ef0332e-a24e-4db0-bbef-ce039a67777c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy Dataframe.\n",
        "df_copy = data.copy()"
      ],
      "metadata": {
        "id": "SLzi4LtPDAPv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-94rWjh-rF70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd5ba65-a8af-43fb-920b-a1485354707e"
      },
      "source": [
        "# Check for Missing Values.\n",
        "print(data.isnull().sum())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAD           0\n",
            "LOAN          0\n",
            "MORTDUE     518\n",
            "VALUE       112\n",
            "REASON      252\n",
            "JOB         279\n",
            "YOJ         515\n",
            "DEROG       708\n",
            "DELINQ      580\n",
            "CLAGE       308\n",
            "NINQ        510\n",
            "CLNO        222\n",
            "DEBTINC    1267\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQD4TtoAq9vQ"
      },
      "source": [
        "## **Delete Rows (or Columns) with Missing Values.**\n",
        "\n",
        "This method is commonly used to handle the null values. Here, we either delete a particular row if it has a null value for a particular feature and a particular column if it has more than $70-75\\%$ of missing values. This method is advised only when there are enough samples in the dataset. One has to make sure that after we have deleted the data, there is no addition of bias. Removing the data will lead to loss of information which will not give the expected results while predicting the output.\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "*   Complete removal of data with missing values results in a robust and highly accurate model.\n",
        "*   Deleting a particular row or a column with no specific information is better since it does not have a high weightage.\n",
        "\n",
        "**Cons:**\n",
        "*   Loss of information and data.\n",
        "*   Works poorly if the percentage of missing values is high (say $30\\%$), compared to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzSprCmAyqhq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b28856-bdd0-476a-f891-7d1660d99889"
      },
      "source": [
        "# Delete Entire Column (Feature) with Missing Values.\n",
        "del df_copy[\"DEBTINC\"]\n",
        "print(df_copy.isnull().sum())\n",
        "print(df_copy.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAD          0\n",
            "LOAN         0\n",
            "MORTDUE    518\n",
            "VALUE      112\n",
            "REASON     252\n",
            "JOB        279\n",
            "YOJ        515\n",
            "DEROG      708\n",
            "DELINQ     580\n",
            "CLAGE      308\n",
            "NINQ       510\n",
            "CLNO       222\n",
            "dtype: int64\n",
            "(5960, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Y3_y7Pu8fH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b4c659-a442-4542-c970-b68bd207641e"
      },
      "source": [
        "# Delete Rows with Missing Values.\n",
        "df_copy.dropna(inplace=True)\n",
        "print(df_copy.isnull().sum())\n",
        "print(df_copy.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAD        0\n",
            "LOAN       0\n",
            "MORTDUE    0\n",
            "VALUE      0\n",
            "REASON     0\n",
            "JOB        0\n",
            "YOJ        0\n",
            "DEROG      0\n",
            "DELINQ     0\n",
            "CLAGE      0\n",
            "NINQ       0\n",
            "CLNO       0\n",
            "dtype: int64\n",
            "(4247, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu955L9x0PL4"
      },
      "source": [
        "## **Impute Missing Values with Mean, Median, and Mode.**\n",
        "\n",
        "This strategy can be applied to a feature that has numeric data like the age of a person or the ticket fare. We can calculate the mean, median, or mode of the feature and replace it with the missing values. This is an approximation that can add variance to the dataset. But the loss of the data can be negated by this method which yields better results compared to the removal of rows and columns. Replacing the above three approximations is a statistical approach for handling the missing values. This method is also called leaking the data while training. Another way is to approximate it with the deviation of neighboring values. This works better if the dataset is linear.\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "*   It is a better approach when the data size is small.\n",
        "*   It can prevent data loss, which results in the removal of the rows and columns.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "*   Imputing the approximations add variance and bias.\n",
        "*   Works poorly compared to other multiple-imputations methods.\n",
        "*   Works only with numerical continuous variables.\n",
        "*   Can cause data leakage.\n",
        "*   Do not factor the covariance between features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhr3LaKF3LTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957cb4e7-5a4f-4c29-f14c-3ed1725c2a90"
      },
      "source": [
        "\"\"\" Imputation Using the Mean Values. \"\"\"\n",
        "\n",
        "# Copy Dataframe.\n",
        "df_mean_impute = data.copy()\n",
        "\n",
        "\"\"\" Replace missing values using Mean Imputation. \"\"\"\n",
        "# df_mean_impute[\"CLNO\"].fillna(df_mean_impute[\"CLNO\"].mean())\n",
        "\n",
        "df_mean_impute = df_mean_impute.fillna(df_mean_impute.mean())\n",
        "print(df_mean_impute.isnull().sum())\n",
        "print(df_mean_impute.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAD          0\n",
            "LOAN         0\n",
            "MORTDUE      0\n",
            "VALUE        0\n",
            "REASON     252\n",
            "JOB        279\n",
            "YOJ          0\n",
            "DEROG        0\n",
            "DELINQ       0\n",
            "CLAGE        0\n",
            "NINQ         0\n",
            "CLNO         0\n",
            "DEBTINC      0\n",
            "dtype: int64\n",
            "(5960, 13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUHPuj679Zz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0295975-01c0-4c5b-bb6e-c9a850601b4f"
      },
      "source": [
        "\"\"\" Imputation Using the Median Values. \"\"\"\n",
        "\n",
        "# Copy Dataframe.\n",
        "df_median_impute = data.copy()\n",
        "\n",
        "\"\"\" Replace missing values using Median Imputation. \"\"\"\n",
        "# df_median_impute[\"CLNO\"].fillna(df_median_impute[\"CLNO\"].mean())\n",
        "\n",
        "df_median_impute = df_median_impute.fillna(df_median_impute.median())\n",
        "print(df_median_impute.isnull().sum())\n",
        "print(df_median_impute.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAD          0\n",
            "LOAN         0\n",
            "MORTDUE      0\n",
            "VALUE        0\n",
            "REASON     252\n",
            "JOB        279\n",
            "YOJ          0\n",
            "DEROG        0\n",
            "DELINQ       0\n",
            "CLAGE        0\n",
            "NINQ         0\n",
            "CLNO         0\n",
            "DEBTINC      0\n",
            "dtype: int64\n",
            "(5960, 13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBEWQrby9xuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc81cc4c-d266-4296-eb54-b5918fc63b9b"
      },
      "source": [
        "\"\"\" Imputation Using the Mode Values. \"\"\"\n",
        "\n",
        "# Copy Dataframe.\n",
        "df_mode_impute = data.copy()\n",
        "\n",
        "\"\"\" Replace missing values using Mode Imputation. \"\"\"\n",
        "# df_mode_impute[\"CLNO\"].fillna(df_mode_impute[\"CLNO\"].mean())\n",
        "\n",
        "df_mode_impute = df_mode_impute.fillna(df_mode_impute.mode())\n",
        "print(df_mode_impute.isnull().sum())\n",
        "print(df_mode_impute.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAD          0\n",
            "LOAN         0\n",
            "MORTDUE    518\n",
            "VALUE      112\n",
            "REASON     252\n",
            "JOB        279\n",
            "YOJ        515\n",
            "DEROG      708\n",
            "DELINQ     580\n",
            "CLAGE      308\n",
            "NINQ       510\n",
            "CLNO       222\n",
            "DEBTINC    226\n",
            "dtype: int64\n",
            "(5960, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7hYzoUr-0jr"
      },
      "source": [
        "## **Imputation Method for Categorical Columns (Assigning An Unique Category):**\n",
        "\n",
        "When missing values is from categorical columns (string or numerical), then the missing values can be replaced with the most frequent category. If the number of missing values is very large, then it can be replaced with a new category.\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "*   Prevent data loss which results in deletion of rows or columns.\n",
        "*   Works well with a small dataset and is easy to implement.\n",
        "*   Negates the loss of data by adding a unique category.\n",
        "*   Fewer possibilities with one extra category, resulting in low variance after one hot encoding — since it is categorical.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "*   Addition of new features to the model while encoding, which may result in poor performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4xhSwvu-5kH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66032002-8112-4849-90fc-99b0a71f8251"
      },
      "source": [
        "\"\"\" Imputation Using (Zero/Constant) Values. \"\"\"\n",
        "\n",
        "# Copy Dataframe.\n",
        "df_constant_impute = data.copy()\n",
        "\n",
        "\"\"\" Replace missing values using Constant Imputation. \"\"\"\n",
        "# df_constant_impute[\"REASON\"].fillna(\"NA\")\n",
        "\n",
        "\"\"\" Replace missing values with a number. \"\"\"\n",
        "df_constant_impute = df_constant_impute.fillna(0)\n",
        "print(df_constant_impute.isnull().sum())\n",
        "print(df_constant_impute.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAD        0\n",
            "LOAN       0\n",
            "MORTDUE    0\n",
            "VALUE      0\n",
            "REASON     0\n",
            "JOB        0\n",
            "YOJ        0\n",
            "DEROG      0\n",
            "DELINQ     0\n",
            "CLAGE      0\n",
            "NINQ       0\n",
            "CLNO       0\n",
            "DEBTINC    0\n",
            "dtype: int64\n",
            "(5960, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QvjrDZ-Flgo"
      },
      "source": [
        "# **Using Algorithms that Support Missing Values.**\n",
        "\n",
        "KNN is a machine learning algorithm that works on the principle of distance measure. This algorithm can be used when there are null values present in the dataset. When the algorithm is applied, KNN considers the missing values by taking the majority of the $K$-nearest values.\n",
        "\n",
        "> [**Imputation of Missing Values**](https://scikit-learn.org/stable/modules/impute.html#nearest-neighbors-imputation)\n",
        "\n",
        "Another algorithm that can be used here is RandomForest. This model produces a robust result because it works well on non-linear and categorical data. It adapts to the data structure taking into consideration of the high variance or the bias, producing better results on large datasets.\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "*  Does not require the creation of a predictive model for each attribute with missing data in the dataset.\n",
        "\n",
        "*  The correlation of the data is neglected.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "*  It is a very time-consuming process, and it can be critical in data mining where large databases are being extracted.\n",
        "\n",
        "*  Choice of distance functions can be Euclidean, Manhattan, etc. which is do not yield a robust result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v41Kh_RVrzT"
      },
      "source": [
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "\n",
        "def imputeF20(X, method=\"none\"):\n",
        "    if method == \"none\":\n",
        "        return pd.DataFrame(X)\n",
        "    if method == \"drop\":\n",
        "        X = X.drop(\"DEBTINC\", axis=1).values\n",
        "        return pd.DataFrame(X)\n",
        "    if method == \"constant\":\n",
        "        imp = SimpleImputer(strategy=\"constant\")\n",
        "    if method == \"mean\":\n",
        "        imp = SimpleImputer(strategy=\"mean\")\n",
        "    if method == \"median\":\n",
        "        imp = SimpleImputer(strategy=\"median\")\n",
        "    if method == \"most_frequent\":\n",
        "        imp = SimpleImputer(strategy=\"most_frequent\")\n",
        "    if method == \"knn\":\n",
        "        imp = KNNImputer(n_neighbors=5)\n",
        "\n",
        "    imp.fit(X)\n",
        "    return pd.DataFrame(imp.transform(X))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4WFJvIdYmj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ce0fd3-244a-4c99-962d-df0c97fbc384"
      },
      "source": [
        "slc = [1, 2, 3, 6, 7, 8, 9, 10, 11]\n",
        "features = pd.DataFrame(data.values[:, slc], data.index, data.columns[slc]).values\n",
        "features_impute = imputeF20(features, \"knn\")\n",
        "\n",
        "print(features_impute.isnull().sum())\n",
        "print(features_impute.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "5    0\n",
            "6    0\n",
            "7    0\n",
            "8    0\n",
            "dtype: int64\n",
            "(5960, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HDWf3oqbjAt"
      },
      "source": [
        "# **DataWig: A framework for learning models to impute missing values in tables.**\n",
        "\n",
        "*  [**Welcome to DataWig’s documentation!**](https://datawig.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "*  [**DataWig Github**](https://github.com/awslabs/datawig)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imputation by Multivariate Imputation by Chained Equation (MICE)**\n",
        "\n",
        "MICE is a method for replacing missing data values in data collection via multiple imputations.\n",
        "\n",
        "> [**MICE**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/)\n",
        "\n",
        "> [**The MICE Algorithm**](https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html)"
      ],
      "metadata": {
        "id": "LW7tOfd-hdVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library.\n",
        "import seaborn as sns\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Load Dataset.\n",
        "data = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Feature Engineering.\n",
        "data = data[[\"survived\", \"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"fare\"]]\n",
        "data[\"sex\"] = [1 if x == \"male\" else 0 for x in data[\"sex\"]]\n",
        "data.head()\n",
        "\n",
        "# Handling Missing Values.\n",
        "imputer = IterativeImputer(\n",
        "    imputation_order=\"ascending\", max_iter=10, random_state=42, n_nearest_features=5\n",
        ")\n",
        "imputed_dataset = imputer.fit_transform(data)"
      ],
      "metadata": {
        "id": "Vv-qyB8AiLVf"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}