{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFDataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [$tf.data.Dataset$](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
        "\n",
        "Represents a potentially large set of elements."
      ],
      "metadata": {
        "id": "aP2dv4VtvWoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsJlS-Zht_Vs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create a $tf.data.Dataset$ from a given list of elements.**"
      ],
      "metadata": {
        "id": "iB84wlapv-Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "elements = [21, 22, -108, 31, -1, 32, 34, 31]\n",
        "tf_dataset = tf.data.Dataset.from_tensor_slices(elements)\n",
        "print(tf_dataset)"
      ],
      "metadata": {
        "id": "1Tn1LGeIv9mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Iterate through $tf.data.Dataset$.**"
      ],
      "metadata": {
        "id": "CeRoCjEjw1Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in tf_dataset:\n",
        "    print(item.numpy())"
      ],
      "metadata": {
        "id": "b662kZfKwuP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Iterate through all elements as NumPy elements.**"
      ],
      "metadata": {
        "id": "WwHX4C63xoLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in tf_dataset.as_numpy_iterator():\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "Gu1hGXpZxmM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Iterate through the first \"$n$\" elements in the $tf.data.Dataset$.**"
      ],
      "metadata": {
        "id": "mYKRnGpiyiDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in tf_dataset.take(3):\n",
        "    print(item.numpy())"
      ],
      "metadata": {
        "id": "Dl9gK95JykLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Filter elements that are greater than 0.**"
      ],
      "metadata": {
        "id": "058a0HvEzHHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dataset = tf_dataset.filter(lambda x: x > 0)\n",
        "for item in tf_dataset.as_numpy_iterator():\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "kBtWkjiZz9rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multiply each element with a value of 10.**"
      ],
      "metadata": {
        "id": "XvfQzO7N1CuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dataset = tf_dataset.map(lambda x: x * 10)\n",
        "for item in tf_dataset.as_numpy_iterator():\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "Hp9eErEP1EUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Shuffle elements in the $tf.data.Dataset$.**"
      ],
      "metadata": {
        "id": "MPnd0yy_3mxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dataset = tf_dataset.shuffle(2)\n",
        "for item in tf_dataset.as_numpy_iterator():\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "TOmFKcr-3mOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Batching elements in the $tf.data.Dataset$.**"
      ],
      "metadata": {
        "id": "VT4vk7Za3qbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item_batch in tf_dataset.batch(2):\n",
        "    print(item_batch.numpy())"
      ],
      "metadata": {
        "id": "ANx_J0Zh3sxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Perform all of the above operations in one shot.**"
      ],
      "metadata": {
        "id": "DgXDex1PFAcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dataset = tf.data.Dataset.from_tensor_slices(elements)\n",
        "\n",
        "tf_dataset = (\n",
        "    tf_dataset.filter(lambda x: x > 0).map(lambda y: y * 10).shuffle(2).batch(2)\n",
        ")\n",
        "\n",
        "for item in tf_dataset.as_numpy_iterator():\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "bPWrt6PbFFMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load CSV Dataset.**"
      ],
      "metadata": {
        "id": "pGtyGvi6I57m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
        "    names=[\n",
        "        \"Length\",\n",
        "        \"Diameter\",\n",
        "        \"Height\",\n",
        "        \"Whole Weight\",\n",
        "        \"Shucked Weight\",\n",
        "        \"Viscera Weight\",\n",
        "        \"Shell Weight\",\n",
        "        \"Age\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "b5G4c3oKJFMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save into .CSV File.\n",
        "data.to_csv(\"abalone_train.csv\")"
      ],
      "metadata": {
        "id": "HIqdxeW6QNJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BLOG:** [**Stop using CSVs for Storage. Pickle is an 80 times faster alternative.**](https://towardsdatascience.com/stop-using-csvs-for-storage-pickle-is-an-80-times-faster-alternative-832041bbc199)"
      ],
      "metadata": {
        "id": "V2tjt2SqQ1TC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save into Pickle File.\n",
        "pd.to_pickle(data, \"train.pkl\")\n",
        "\n",
        "# Read the Pickle File.\n",
        "data = pd.read_pickle(\"train.pkl\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "q9XWn8QgTFLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Dump Pickle File.\n",
        "with open(\"abalone_train.pkl\", \"wb\") as f:\n",
        "    pickle.dump(\"abalone_train.csv\", f)\n",
        "\n",
        "# Load Pickle File.\n",
        "with open(\"abalone_train.pkl\", \"rb\") as f:\n",
        "    df = pickle.load(f)"
      ],
      "metadata": {
        "id": "lgEONWJAQ1dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Images.**\n",
        "\n",
        "> [**Kaggle Dataset**](https://www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset)"
      ],
      "metadata": {
        "id": "mWrQxtrZvvJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle.\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "K_1y9JrnV1MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Files Upload.\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "r3E2pKFswCTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Kaggle Folder.\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# Copy the kaggle.json to the folder created.\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Permission for the json file to act.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "23drJLolwF-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Download.\n",
        "!kaggle datasets download -d shaunthesheep/microsoft-catsvsdogs-dataset"
      ],
      "metadata": {
        "id": "_kYPUP81wGry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Dataset.\n",
        "!unzip microsoft-catsvsdogs-dataset.zip"
      ],
      "metadata": {
        "id": "Y_b9Tf_kwI6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Images using $tf.data.Dataset$.**"
      ],
      "metadata": {
        "id": "SZbEe2NOxnuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Images to tf.data.Dataset.\n",
        "images_ds = tf.data.Dataset.list_files(\"PetImages/*/*\", shuffle=False)"
      ],
      "metadata": {
        "id": "FRWs4tw-xv2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Count.\n",
        "image_count = len(images_ds)\n",
        "print(image_count)"
      ],
      "metadata": {
        "id": "5B8GUAXUxv3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle and Iterate through the first \"n\" elements in the tf.data.Dataset.\n",
        "images_ds = images_ds.shuffle(500)\n",
        "for file in images_ds.take(3):\n",
        "    print(file.numpy())"
      ],
      "metadata": {
        "id": "vD_wPM8Jxv4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = [\"Cat\", \"Dog\"]"
      ],
      "metadata": {
        "id": "AKEiSU_f0Q-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Dataset into Training and Test Set.\n",
        "train_size = int(image_count * 0.8)\n",
        "\n",
        "train_ds = images_ds.take(train_size)\n",
        "test_ds = images_ds.skip(train_size)\n",
        "\n",
        "print(len(train_ds), len(test_ds))"
      ],
      "metadata": {
        "id": "xmu4feYq0f_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(file_path):\n",
        "    parts = tf.strings.split(file_path, os.path.sep)\n",
        "    return parts[-2]\n",
        "\n",
        "\n",
        "def process_image(file_path):\n",
        "    label = get_label(file_path)\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    img = tf.image.resize(img, [128, 128])\n",
        "    return img, label"
      ],
      "metadata": {
        "id": "SiEtUj-y0ikk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_label(\"/content/PetImages/Dog/10037.jpg\")"
      ],
      "metadata": {
        "id": "ZMW6zOFf9MfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = process_image(\"/content/PetImages/Dog/10037.jpg\")\n",
        "img.numpy()[:2]"
      ],
      "metadata": {
        "id": "OOn3olWM9rOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(process_image)\n",
        "test_ds = test_ds.map(process_image)"
      ],
      "metadata": {
        "id": "b8JcKyE2-hnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in train_ds.take(1):\n",
        "    print(\"****\", image)\n",
        "    print(\"****\", label)"
      ],
      "metadata": {
        "id": "RxxVt1h0-nHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the Image Values.\n",
        "def scale(image, label):\n",
        "    return image / 255, label\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(scale)\n",
        "\n",
        "for image, label in train_ds.take(5):\n",
        "    print(\"****Image: \", image.numpy()[0][0])\n",
        "    print(\"****Label: \", label.numpy())"
      ],
      "metadata": {
        "id": "HqxcQia8CU7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fVGikhVGyVJ"
      },
      "source": [
        "<h3  align=\"center\" style=\"color:blue\"><b>TF Data Input Pipeline: Exercise Solution</b></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2inA_XCkGyVK"
      },
      "source": [
        "Movie reviews are present as individual text files (one file per review) in the review folder.\n",
        "\n",
        "Folder structure looks like this,\n",
        "\n",
        "**[reviews](https://github.com/codebasics/deep-learning-keras-tf-tutorial/tree/master/44_tf_data_pipeline/Exercise/reviews)**\n",
        "\n",
        "    |__ positive\n",
        "        |__pos_1.txt\n",
        "        |__pos_2.txt\n",
        "        |__pos_3.txt\n",
        "\n",
        "    |__ negative\n",
        "        |__neg_1.txt\n",
        "        |__neg_2.txt\n",
        "        |__neg_3.txt\n",
        "\n",
        "\n",
        "We need to read these reviews using $tf.data.Dataset$ and perform the following transformations.\n",
        "\n",
        "1.    Read text review and generate a label from the folder name. The dataset should have review text and label as a tuple.\n",
        "\n",
        "2.   Filter blank text review. Two files are blank in this dataset.\n",
        "\n",
        "3.   Do all of the above transformations in a single line of code. Also, shuffle all the reviews."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve and View review file paths in a TensorFlow Dataset.\n",
        "reviews_ds = tf.data.Dataset.list_files(\"reviews/*/*\", shuffle=False)"
      ],
      "metadata": {
        "id": "qLQnEDgiJpbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in reviews_ds:\n",
        "    print(file.numpy())"
      ],
      "metadata": {
        "id": "4Mm3uS8iJ5OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_review_and_label(file_path):\n",
        "    return tf.io.read_file(file_path), tf.strings.split(file_path, os.path.sep)[-2]"
      ],
      "metadata": {
        "id": "lmPOjKfkNiAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extract review text from these files. Extract the label from the folder name.**"
      ],
      "metadata": {
        "id": "LgrVhJzeO-T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_ds_1 = reviews_ds.map(extract_review_and_label)\n",
        "\n",
        "for review, label in reviews_ds_1:\n",
        "    print(\"Review: \", review.numpy()[:50])\n",
        "    print(\"Label: \", label.numpy())"
      ],
      "metadata": {
        "id": "6ee3du57NiHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Filter Blank Reviews.**"
      ],
      "metadata": {
        "id": "Mjos--RBPNki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_ds_2 = reviews_ds_1.filter(lambda review, label: review != \"\")\n",
        "\n",
        "for review, label in reviews_ds_2.as_numpy_iterator():\n",
        "    print(\"Review: \", review[:50])\n",
        "    print(\"Label: \", label)"
      ],
      "metadata": {
        "id": "CHrPl-AwPQJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Perform $map()$, $filter()$, and $shuffle()$ all in a single line of code.**"
      ],
      "metadata": {
        "id": "q9Im1nnDPbFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_ds = (\n",
        "    reviews_ds.map(extract_review_and_label)\n",
        "    .filter(lambda review, label: review != \"\")\n",
        "    .shuffle(2)\n",
        ")\n",
        "\n",
        "for review, label in final_ds.as_numpy_iterator():\n",
        "    print(\"Review:\", review[:50])\n",
        "    print(\"Label:\", label)"
      ],
      "metadata": {
        "id": "rMtq70_fPbSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [**Tensorflow Input Pipeline | TF Dataset**](https://www.youtube.com/watch?v=VFEOskzhhbc&list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&index=44)"
      ],
      "metadata": {
        "id": "LYFN_ShMbd5h"
      }
    }
  ]
}