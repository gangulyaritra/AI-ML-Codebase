{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW3vtotOukqy"
      },
      "source": [
        "# **Tuning Hyperparameters of An Artificial Neural Network Using Keras Tuner.**\n",
        "\n",
        "Hyperparameter tuning is the process of searching optimal set of hyperparameters. It is very difficult to find the optimal set of hyperparameters manually, so certain algorithms make our hyperparameter search easier. Grid search is one of the algorithms that perform an exhaustive search, which is time-consuming by nature. An alternative is the Random Search algorithm that randomly searches the hyperparameter search space, but doesn't guarantee a globally optimal solution. The algorithms which are more likely to provide globally optimal solutions are Bayesian optimization, Hyperband, and Hyperparameter optimization using Genetic algorithms.\n",
        "\n",
        "**Hyperparameters of an Artificial Neural Network are:**\n",
        "\n",
        "*   Number of layers to choose.\n",
        "*   Number of neurons in a layer to choose.\n",
        "*   Choice of the optimization function.\n",
        "*   Choice of the learning rate for optimization function.\n",
        "*   Choice of the loss function.\n",
        "*   Choice of metrics.\n",
        "*   Choice of activation function.\n",
        "*   Choice of layer weight initialization.\n",
        "\n",
        "## **KerasTuner**\n",
        "\n",
        "[**KerasTuner**](https://keras.io/keras_tuner/) is an easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search. Easily configure your search space with a define-by-run syntax, then leverage one of the available search algorithms to find the best hyperparameter values for your models. KerasTuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms.\n",
        "\n",
        "> [Introduction to the Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner)\n",
        "\n",
        "\n",
        "Keras Tuner is an open-source python library developed exclusively for tuning the hyperparameters of Artificial Neural Networks. Keras tuner currently supports four types of tuners or algorithms.\n",
        "\n",
        "*   **Bayesian Optimization**\n",
        "*   **Hyperband**\n",
        "*   **Sklearn**\n",
        "*   **Random Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQIajsyHsdjF"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpohU2IRyPFm"
      },
      "outputs": [],
      "source": [
        "# Import Library.\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "TRAIN_DATA_PATH = \"/content/sample_data/california_housing_train.csv\"\n",
        "TEST_DATA_PATH = \"/content/sample_data/california_housing_test.csv\"\n",
        "TARGET_NAME = \"median_house_value\"\n",
        "\n",
        "# Load Dataset.\n",
        "train_data = pd.read_csv(TRAIN_DATA_PATH)\n",
        "test_data = pd.read_csv(TEST_DATA_PATH)\n",
        "\n",
        "# Split the data into features and target sets.\n",
        "X_train, y_train = train_data.drop(TARGET_NAME, axis=1), train_data[TARGET_NAME]\n",
        "X_test, y_test = test_data.drop(TARGET_NAME, axis=1), test_data[TARGET_NAME]\n",
        "\n",
        "# Feature Scaling.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JROi4TvM4MFI"
      },
      "source": [
        "**Let's fine-tune the model with a Keras-tuner. The following tuner gets defined with the model builder function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZJ8Stks3TpS",
        "outputId": "f93da8c1-2d7f-49c9-a1c0-24bcb5a4ed5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 00m 41s]\n",
            "val_mean_squared_logarithmic_error: 0.09914283454418182\n",
            "\n",
            "Best val_mean_squared_logarithmic_error So Far: 0.09738881886005402\n",
            "Total elapsed time: 00h 05m 42s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ],
      "source": [
        "import kerastuner as kt\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Tune the number of units in the first Dense layer. Choose an optimal value between 32-512.\n",
        "    hp_units1 = hp.Int(\"units1\", min_value=32, max_value=512, step=16)\n",
        "    hp_units2 = hp.Int(\"units2\", min_value=32, max_value=512, step=16)\n",
        "    hp_units3 = hp.Int(\"units3\", min_value=32, max_value=512, step=16)\n",
        "    model.add(tf.keras.layers.Dense(units=hp_units1, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(units=hp_units2, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(units=hp_units3, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(1, kernel_initializer=\"normal\", activation=\"linear\"))\n",
        "\n",
        "    # Tune the learning rate for the optimizer. Choose an optimal value from 0.01, 0.001, or 0.0001.\n",
        "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    # Compile the Model.\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "        loss=\"MeanSquaredLogarithmicError\",\n",
        "        metrics=[\"MeanSquaredLogarithmicError\"],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# HyperBand Algorithm from Keras Tuner.\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective=\"val_mean_squared_logarithmic_error\",\n",
        "    max_epochs=10,\n",
        "    directory=\"keras_tuner_dir\",\n",
        "    project_name=\"keras_tuner_demo\",\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    epochs=20,\n",
        "    validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2kApbQNZb5Q",
        "outputId": "5bc83bd7-b34d-4b62-8107-4ffc19747a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "units1 160\n",
            "units2 464\n",
            "units3 432\n",
            "learning_rate 0.01\n"
          ]
        }
      ],
      "source": [
        "# The best hyper-parameters can be fetched using the method `get_best_hyperparameters()` in the tuner instance.\n",
        "for h_param in [f\"units{i}\" for i in range(1, 4)] + [\"learning_rate\"]:\n",
        "    print(h_param, tuner.get_best_hyperparameters()[0].get(h_param))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJDt2oonahlw",
        "outputId": "78e389b4-779d-4084-c29b-cca76befde66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (17000, 160)              1440      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (17000, 464)              74704     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (17000, 432)              200880    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (17000, 1)                433       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 277,457\n",
            "Trainable params: 277,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
          ]
        }
      ],
      "source": [
        "# Select the Best Model which is saved in the tuner instance.\n",
        "best_model = tuner.get_best_models()[0]\n",
        "best_model.build(X_train_scaled.shape)\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7HC2hsgaNsC",
        "outputId": "f159f944-672f-4700-afcd-df49a989e0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "213/213 [==============================] - 2s 8ms/step - loss: 0.0856 - mean_squared_logarithmic_error: 0.0856 - val_loss: 0.1016 - val_mean_squared_logarithmic_error: 0.1016\n",
            "Epoch 2/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0849 - mean_squared_logarithmic_error: 0.0849 - val_loss: 0.1034 - val_mean_squared_logarithmic_error: 0.1034\n",
            "Epoch 3/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0847 - mean_squared_logarithmic_error: 0.0847 - val_loss: 0.1041 - val_mean_squared_logarithmic_error: 0.1041\n",
            "Epoch 4/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0835 - mean_squared_logarithmic_error: 0.0835 - val_loss: 0.0955 - val_mean_squared_logarithmic_error: 0.0955\n",
            "Epoch 5/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0832 - mean_squared_logarithmic_error: 0.0832 - val_loss: 0.0967 - val_mean_squared_logarithmic_error: 0.0967\n",
            "Epoch 6/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0826 - mean_squared_logarithmic_error: 0.0826 - val_loss: 0.0930 - val_mean_squared_logarithmic_error: 0.0930\n",
            "Epoch 7/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0813 - mean_squared_logarithmic_error: 0.0813 - val_loss: 0.0960 - val_mean_squared_logarithmic_error: 0.0960\n",
            "Epoch 8/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0810 - mean_squared_logarithmic_error: 0.0810 - val_loss: 0.1043 - val_mean_squared_logarithmic_error: 0.1043\n",
            "Epoch 9/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0803 - mean_squared_logarithmic_error: 0.0803 - val_loss: 0.0954 - val_mean_squared_logarithmic_error: 0.0954\n",
            "Epoch 10/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0792 - mean_squared_logarithmic_error: 0.0792 - val_loss: 0.0942 - val_mean_squared_logarithmic_error: 0.0942\n",
            "Epoch 11/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0779 - mean_squared_logarithmic_error: 0.0779 - val_loss: 0.0942 - val_mean_squared_logarithmic_error: 0.0942\n",
            "Epoch 12/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0775 - mean_squared_logarithmic_error: 0.0775 - val_loss: 0.1035 - val_mean_squared_logarithmic_error: 0.1035\n",
            "Epoch 13/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0768 - mean_squared_logarithmic_error: 0.0768 - val_loss: 0.0988 - val_mean_squared_logarithmic_error: 0.0988\n",
            "Epoch 14/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0749 - mean_squared_logarithmic_error: 0.0749 - val_loss: 0.1055 - val_mean_squared_logarithmic_error: 0.1055\n",
            "Epoch 15/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0742 - mean_squared_logarithmic_error: 0.0742 - val_loss: 0.0912 - val_mean_squared_logarithmic_error: 0.0912\n",
            "Epoch 16/20\n",
            "213/213 [==============================] - 2s 9ms/step - loss: 0.0722 - mean_squared_logarithmic_error: 0.0722 - val_loss: 0.0924 - val_mean_squared_logarithmic_error: 0.0924\n",
            "Epoch 17/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0719 - mean_squared_logarithmic_error: 0.0719 - val_loss: 0.0920 - val_mean_squared_logarithmic_error: 0.0920\n",
            "Epoch 18/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0704 - mean_squared_logarithmic_error: 0.0704 - val_loss: 0.1165 - val_mean_squared_logarithmic_error: 0.1165\n",
            "Epoch 19/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0698 - mean_squared_logarithmic_error: 0.0698 - val_loss: 0.0850 - val_mean_squared_logarithmic_error: 0.0850\n",
            "Epoch 20/20\n",
            "213/213 [==============================] - 1s 7ms/step - loss: 0.0704 - mean_squared_logarithmic_error: 0.0704 - val_loss: 0.0912 - val_mean_squared_logarithmic_error: 0.0912\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c7582d3d0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit the Best Model.\n",
        "best_model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-2acnT49MHw"
      },
      "source": [
        "**Alternatively, we can define the hyper model by subclassing HyperModel class in the Keras Tuner.**\n",
        "\n",
        "**HyperModel is a keras tuner class that lets you define the model with a searchable space and build it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyEW2SW586fZ"
      },
      "outputs": [],
      "source": [
        "# Create a class that inherits from kerastuner.HyperModel\n",
        "from kerastuner import HyperModel\n",
        "\n",
        "\n",
        "class RegressionHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = tf.keras.Sequential()\n",
        "\n",
        "        model.add(\n",
        "            tf.keras.layers.Dense(\n",
        "                units=hp.Int(\"units\", 8, 64, 4, default=8),\n",
        "                activation=hp.Choice(\n",
        "                    \"dense_activation\",\n",
        "                    values=[\"relu\", \"tanh\", \"sigmoid\"],\n",
        "                    default=\"relu\",\n",
        "                ),\n",
        "                input_shape=self.input_shape,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        model.add(\n",
        "            tf.keras.layers.Dense(\n",
        "                units=hp.Int(\"units\", 16, 64, 4, default=16),\n",
        "                activation=hp.Choice(\n",
        "                    \"dense_activation\",\n",
        "                    values=[\"relu\", \"tanh\", \"sigmoid\"],\n",
        "                    default=\"relu\",\n",
        "                ),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        model.add(\n",
        "            tf.keras.layers.Dropout(\n",
        "                hp.Float(\"dropout\", min_value=0.0, max_value=0.1, default=0.005, step=0.01)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "        # Tune the learning rate for the optimizer. Choose an optimal value from 0.01, 0.001, or 0.0001.\n",
        "        hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "        # Compile the Model.\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "            loss=\"mse\",\n",
        "            metrics=[\"mse\"],\n",
        "        )\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwRmUHUyGjJg"
      },
      "outputs": [],
      "source": [
        "# Instantiate HyperModel.\n",
        "hypermodel = RegressionHyperModel(input_shape=(X_train_scaled.shape[1],))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHhP0p_DIJZh"
      },
      "source": [
        "### **Random Search**\n",
        "\n",
        "Random Search is a hyperparameter tuning method which randomly tries a combination of hyperparameters from a given search space. To use this method in keras tuner, let's define a tuner using one of the available Tuners. Here's a full list of [Tuners](https://keras.io/api/keras_tuner/tuners/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARnLOkLGI4En",
        "outputId": "095dd92b-2dfb-4f6c-f2a0-7bd7bb6fb561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 00m 38s]\n",
            "mse: 48688111616.0\n",
            "\n",
            "Best mse So Far: 4101500032.0\n",
            "Total elapsed time: 00h 06m 29s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 4365881344.0000 - mse: 4365881344.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[4365881344.0, 4365881344.0]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# RandomSearch Algorithm from Keras Tuner.\n",
        "tuner_rs = kt.RandomSearch(\n",
        "    hypermodel, objective=\"mse\", max_trials=10, executions_per_trial=2, seed=42\n",
        ")\n",
        "\n",
        "# Run the random search tuner using the search method.\n",
        "tuner_rs.search(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    epochs=20,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "\n",
        "# Select the best combination of hyperparameters the tuner had tried and evaluate.\n",
        "best_model = tuner_rs.get_best_models(num_models=1)[0]\n",
        "best_model.evaluate(X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEofrdSHLpP6"
      },
      "source": [
        "### **Hyperband**\n",
        "\n",
        "Hyperband is based on the algorithm by Li et. al. It optimizes random search methods through adaptive resource allocation and early-stopping. Hyperband first runs random hyperparameter configurations for one iteration or two. In the next step, this algorithm selects the set of configurations that performs well and finally continues tuning the best performers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MRyoipJMfGB",
        "outputId": "46e20885-271f-4ba2-e66c-18e26c8a42e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 00m 19s]\n",
            "mse: 50970540032.0\n",
            "\n",
            "Best mse So Far: 4313776896.0\n",
            "Total elapsed time: 00h 05m 05s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 4558466048.0000 - mse: 4558466048.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[4558466048.0, 4558466048.0]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Hyperband Algorithm from Keras Tuner.\n",
        "tuner_hb = kt.Hyperband(\n",
        "    hypermodel,\n",
        "    max_epochs=10,\n",
        "    objective=\"mse\",\n",
        "    seed=42,\n",
        "    executions_per_trial=2,\n",
        "    directory=\"tuner_dir\",\n",
        "    project_name=\"hyperband_tuner_demo\",\n",
        ")\n",
        "\n",
        "# Run the Hyperband tuner using the search method.\n",
        "tuner_hb.search(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    epochs=20,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "\n",
        "# Select the best combination of hyperparameters the tuner had tried and evaluate.\n",
        "best_model = tuner_hb.get_best_models(num_models=1)[0]\n",
        "best_model.evaluate(X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWXXa19JN5A7"
      },
      "source": [
        "### **Bayesian Optimization**\n",
        "\n",
        "Bayesian Optimization is a probabilistic model that maps the hyperparameters to a probability score on the objective function. Unlike Random Search and Hyperband models, Bayesian Optimization keeps track of its past evaluation results and uses it to build the probability model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_Ldxh5tNWUX",
        "outputId": "96964cb4-53fa-4ea1-f123-622fa26f30bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 00m 42s]\n",
            "mse: 3989895552.0\n",
            "\n",
            "Best mse So Far: 3989895552.0\n",
            "Total elapsed time: 00h 06m 37s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "94/94 [==============================] - 0s 1ms/step - loss: 4333397504.0000 - mse: 4333397504.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[4333397504.0, 4333397504.0]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Bayesian Optimization from Keras Tuner.\n",
        "tuner_bo = kt.BayesianOptimization(\n",
        "    hypermodel,\n",
        "    objective=\"mse\",\n",
        "    max_trials=10,\n",
        "    seed=42,\n",
        "    executions_per_trial=2,\n",
        "    directory=\"tuner_dir\",\n",
        "    project_name=\"bayesian_tuner_demo\",\n",
        ")\n",
        "\n",
        "# Run the Bayesian Optimization using the search method.\n",
        "tuner_bo.search(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    epochs=20,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "\n",
        "# Select the best combination of hyperparameters the tuner had tried and evaluate.\n",
        "best_model = tuner_bo.get_best_models(num_models=1)[0]\n",
        "best_model.evaluate(X_test_scaled, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Keras Tuner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('MyVenv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d0f32d21cbf5e0e6bd096f9d1f92dfc448e8bfd417c7697128b2a4e0c0cd030f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}