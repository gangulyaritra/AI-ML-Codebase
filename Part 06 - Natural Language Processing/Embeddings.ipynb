{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.7 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Embeddings.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dm7AbOS4nM6"
      },
      "source": [
        "# **Embeddings**\n",
        "\n",
        "An embedding is a relatively low-dimensional space into which we can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twBvaVNuxw7L"
      },
      "source": [
        "# Embedding Layers in Keras\n",
        "\n",
        "[**Embedding Layers**](https://keras.io/layers/embeddings/) are a handy feature of Keras that allows the program to automatically insert additional information into the data flow of the neural network. In the previous section, we saw that $Word2vec$ could expand words to a 300 dimension vector.  An embedding layer would allow us to insert these 300-dimension vectors in the place of word indexes automatically.\n",
        "\n",
        "Programmers often use embedding layers with Natural Language Processing (NLP); however, they can be used in any instance where we wish to insert a lengthier vector in an index value place. In some ways, we can think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions provide more information to the model and provide a better score.\n",
        "\n",
        "### Simple Embedding Layer Example\n",
        "\n",
        "* `input_dim` = How large is the vocabulary? This parameter is the number of items in our \"lookup table.\"\n",
        "\n",
        "* `output_dim` = How many numbers are in the vector that we wish to return?\n",
        "\n",
        "* `input_length` = How many items are in the input feature vector that we need to transform?\n",
        "\n",
        "Now we create a neural network with a vocabulary size of $10$, which will reduce those values between $0-9$ to $4$ number vectors. Each feature vector coming in will have two such features. This neural network does nothing more than passing the embedding onto the output. But it does let us see what the embedding is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92J2mcIxxw7M",
        "outputId": "3bab02d9-f737-496e-ef4d-ce94a14ce59c"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np\n",
        "\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=10, output_dim=4, input_length=2)\n",
        "model.add(embedding_layer)\n",
        "model.compile(\"adam\", \"mse\")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 2, 4)              40        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40\n",
            "Trainable params: 40\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4_ii2OLxw7N"
      },
      "source": [
        "For this neural network, which is just an embedding layer, the input is a vector of size 2. These two inputs are integer numbers from 0 to 9 (corresponding to the requested `input_dim` quantity of 10 values). Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes from the embedded lookup table that contains four amounts (`output_dim`) for each of the 10 (`input_dim`) possible integer values for the two inputs. The output is 2 (`input_length`) length 4 (`output_dim`) vectors, resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.\n",
        "\n",
        "Now, let us query the neural network with two rows. The input is two integer values, as was specified when we created the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ObAKPvcxw7O",
        "outputId": "1a46a9ed-e0d0-4b33-c090-7f97615fc8e2"
      },
      "source": [
        "input_data = np.array([[1, 2]])\n",
        "\n",
        "pred = model.predict(input_data)\n",
        "\n",
        "print(input_data.shape)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2)\n",
            "[[[ 0.00410897 -0.03215675 -0.04756094  0.01548951]\n",
            "  [ 0.01074568 -0.04540538 -0.02540435  0.04160034]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mri2UEVxw7O"
      },
      "source": [
        "Here we see two length-4 vectors that Keras looked up for each of the input integers. Recall that Python arrays are zero-based. Keras replaced the value of 1 with the second row of the $10 \\times 4$ lookup matrix. Similarly, Keras replaced the value of 2 with the third row of the lookup matrix. The following code displays the lookup matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the correct row from the lookup table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1eAigx1xw7O",
        "outputId": "b95d813b-a157-46dd-81e3-4a4361f34efb"
      },
      "source": [
        "embedding_layer.get_weights()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.02190864, -0.03589261,  0.04029857, -0.02416231],\n",
              "        [ 0.00410897, -0.03215675, -0.04756094,  0.01548951],\n",
              "        [ 0.01074568, -0.04540538, -0.02540435,  0.04160034],\n",
              "        [-0.01743712, -0.0209429 ,  0.04248278, -0.0130057 ],\n",
              "        [-0.02578279,  0.00065221,  0.03479834, -0.03152712],\n",
              "        [ 0.01642852, -0.01104081,  0.01013689, -0.04970313],\n",
              "        [-0.04276519,  0.03464342, -0.00210343, -0.01458585],\n",
              "        [-0.0394882 , -0.02345122, -0.00465295, -0.01985071],\n",
              "        [ 0.01859817,  0.02167213, -0.03111919,  0.04176745],\n",
              "        [-0.0026991 ,  0.02542211,  0.04700674, -0.00861781]],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I55UTyKhxw7O"
      },
      "source": [
        "The values above are random parameters that Keras generated as starting points. Generally, we will either transfer an embedding or train these random values into something useful. The next section demonstrates how to embed a hand-coded embedding.\n",
        "\n",
        "### Transferring An Embedding\n",
        "\n",
        "Now, we see how to hard-code an embedding lookup that performs a simple one-hot encoding.  One-hot encoding would transform the input integer values of 0, 1, and 2 to the vectors $[1,0,0]$, $[0,1,0]$, and $[0,0,1]$ respectively. The following code replaced the random lookup values in the embedding layer with this one-hot coding-inspired lookup table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taoI2H-uxw7O"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np\n",
        "\n",
        "embedding_lookup = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=3, output_dim=3, input_length=2)\n",
        "model.add(embedding_layer)\n",
        "model.compile(\"adam\", \"mse\")\n",
        "\n",
        "embedding_layer.set_weights([embedding_lookup])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpwzzVacxw7P"
      },
      "source": [
        "We have the following parameters to the Embedding layer:\n",
        "    \n",
        "* `input_dim` = 3: There are three different integer categorical values allowed.\n",
        "\n",
        "* `output_dim` = 3: Per one-hot encoding, three columns represent a categorical value with three possible values.\n",
        "\n",
        "* `input_length` = 2: The input vector has two of these categorical values.\n",
        "\n",
        "Now we query the neural network with two categorical values to see the lookup performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM1iFULbxw7P",
        "outputId": "41bd943e-c908-4cc1-9d1f-28462d12c1be"
      },
      "source": [
        "input_data = np.array([[0, 1]])\n",
        "\n",
        "pred = model.predict(input_data)\n",
        "\n",
        "print(input_data.shape)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 2)\n",
            "[[[1. 0. 0.]\n",
            "  [0. 1. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8PrF9_dxw7P"
      },
      "source": [
        "The given output shows that we provided the program with two rows from the one-hot encoding table. This encoding is a correct one-hot encoding for the values 0 and 1, where there are up to 3 unique values possible. \n",
        "\n",
        "The next section demonstrates how to train this embedding lookup table.\n",
        "\n",
        "### Training an Embedding\n",
        "\n",
        "First, we make use of the following imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpAuyCEExw7P"
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Embedding, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OghN3j-xw7Q"
      },
      "source": [
        "We create a neural network that classifies restaurant reviews according to positive or negative. This neural network can accept strings as input, such as given here. This code also includes positive or negative labels for each review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1ez6xUixw7Q"
      },
      "source": [
        "# Define 10 resturant reviews.\n",
        "reviews = [\n",
        "    \"Never coming back!\",\n",
        "    \"Horrible service\",\n",
        "    \"Rude waitress\",\n",
        "    \"Cold food.\",\n",
        "    \"Horrible food!\",\n",
        "    \"Awesome\",\n",
        "    \"Awesome service!\",\n",
        "    \"Rocks!\",\n",
        "    \"poor work\",\n",
        "    \"Couldn't have done better\",\n",
        "]\n",
        "\n",
        "# Define labels (1 = negative, 0 = positive)\n",
        "labels = array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAXOmVOOxw7Q"
      },
      "source": [
        "We define a vocabulary size of 50 words. Though we do not have 50 words, it is okay to use a value larger than needed. If there are more than 50 words, the least frequently used words in the training set are automatically dropped by the embedding layer during training. For input, we one-hot encode the strings. Note that we use the TensorFlow one-hot encoding method here rather than Scikit-Learn. Scikit-learn would expand these strings to the 0's and 1's as we would typically see for dummy variables. TensorFlow translates all of the words to index values and replaces each word with that index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f0ufHFxxw7Q",
        "outputId": "22443bd2-b653-473a-a561-5cae6bbf0b5c"
      },
      "source": [
        "VOCAB_SIZE = 50\n",
        "encoded_reviews = [one_hot(d, VOCAB_SIZE) for d in reviews]\n",
        "print(f\"Encoded reviews: {encoded_reviews}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded reviews: [[5, 30, 25], [21, 42], [21, 38], [43, 26], [21, 26], [8], [8, 42], [12], [2, 7], [3, 11, 36, 48]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETdY9ibrxw7Q"
      },
      "source": [
        "The program one-hot encodes these reviews to word indexes; however, their lengths are different. We pad these reviews to 4 words and truncate any words beyond the fourth word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GPFY-Rmxw7Q",
        "outputId": "6103db83-c7b2-4a13-b179-58b8e6f6c4c8"
      },
      "source": [
        "MAX_LENGTH = 4\n",
        "\n",
        "padded_reviews = pad_sequences(encoded_reviews, maxlen=MAX_LENGTH, padding=\"post\")\n",
        "\n",
        "print(padded_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5 30 25  0]\n",
            " [21 42  0  0]\n",
            " [21 38  0  0]\n",
            " [43 26  0  0]\n",
            " [21 26  0  0]\n",
            " [ 8  0  0  0]\n",
            " [ 8 42  0  0]\n",
            " [12  0  0  0]\n",
            " [ 2  7  0  0]\n",
            " [ 3 11 36 48]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyyLdv80xw7R"
      },
      "source": [
        "Each review is padded by appending zeros at the end, as specified by the $padding=\"post\"$ setting.\n",
        "\n",
        "Next, we create a neural network to learn to classify these reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84BCG6fVxw7R",
        "outputId": "b2dffa6e-b831-4834-9728-d7d37422905c"
      },
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(VOCAB_SIZE, 8, input_length=MAX_LENGTH)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 4, 8)              400       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5YVG2vTxw7R"
      },
      "source": [
        "This network accepts four integer inputs that specify the indexes of a padded movie review. The first embedding layer converts these four indexes into four vectors of length 8. These vectors come from the lookup table that contains 50 (`VOCAB_SIZE`) rows of vectors of length 8. This encoding is evident by the 400 (8 times 50) parameters in the embedding layer. The size of the output from the embedding layer is 32 (4 words expressed as 8-number embedded vectors). A single output neuron is connected to the embedding layer by 33 weights (32 from the embedding layer and a single bias neuron). Because this is a single-class classification network, we use the sigmoid activation function and `binary_crossentropy`.\n",
        "\n",
        "The program now trains the neural network. Both the embedding lookup and dense 33 weights are updated to produce a better score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im2oNsRwxw7R"
      },
      "source": [
        "# Fit the Model.\n",
        "model.fit(padded_reviews, labels, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCuxzLO5xw7R"
      },
      "source": [
        "We can see the learned embeddings. Think of each word's vector as a location in the 8 dimension space where words associated with positive reviews are close to other words with positive reviews. Similarly, training places negative reviews close to each other. In addition to the training setting these embeddings, the 33 weights between the embedding layer and output neuron similarly learn to transform these embeddings into an actual prediction. We can see these embeddings here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-47IsFaXxw7R"
      },
      "source": [
        "print(embedding_layer.get_weights()[0].shape)\n",
        "print(embedding_layer.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_gZgfv2xw7S"
      },
      "source": [
        "We can now evaluate this neural network's accuracy, including both the embeddings and the learned Dense Layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn9q5oEbxw7S",
        "outputId": "75ddd499-efaf-4e67-cc4c-e76b4b1d773b"
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_reviews, labels, verbose=1)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 370ms/step - loss: 0.4339 - accuracy: 1.0000\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Yosqifxw7S",
        "outputId": "138dd75f-5b6a-4145-f734-cc551c23b6cb"
      },
      "source": [
        "print(f\"Log-loss: {loss}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log-loss: 0.43393024802207947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvIKBM0qxw7S"
      },
      "source": [
        "However, the loss is not perfect, meaning that even though the predicted probabilities indicated a correct prediction in every case, the program did not achieve absolute confidence in each correct answer.  The lack of confidence was likely due to the small amount of noise (previously discussed) in the dataset.  Additionally, the fact that some words appeared in both positive and negative reviews contributed to this lack of absolute certainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOzZbxX-HK-w"
      },
      "source": [
        "# **Word Embeddings**\n",
        "\n",
        "*Word embeddings is a technique where individual words get transformed into a numerical representation of the word (i.e., a vector). Each word gets mapped to one vector, and this vector is then learned in a way that resembles a neural network. The vectors try to capture various characteristics of that word with regard to the overall text. These characteristics can include the semantic relationship of the word, definitions, context, etc. With these numerical representations, we can do many things like identify similarities or dissimilarities between words.*\n",
        "\n",
        "Word Embeddings [**[Wikipedia]**](https://en.wikipedia.org/wiki/Word_embedding) is an approach for representing words and documents. Word Embedding is a numeric vector input that represents a word in a lower-dimensional space. It allows words with similar meanings to have a similar representation. It can also approximate meaning. A word vector with 50 values can represent 50 unique features.\n",
        "\n",
        "### **Goals of Word Embeddings**\n",
        "\n",
        "*   To reduce dimensionality.\n",
        "*   To use a word to predict the words around it.\n",
        "*   Inter-word semantics must be captured.\n",
        "\n",
        "#### **How are Word Embeddings used?**\n",
        "\n",
        "*   Word Embeddings are used as input to machine learning models. Take the words $\\rightarrow$ Give their numeric representation $\\rightarrow$ Use in training or inference.\n",
        "\n",
        "*   To represent or visualize any underlying patterns of usage in the corpus that was used to train them.\n",
        "\n",
        "### **Implementations of Word Embeddings**\n",
        "\n",
        "Word Embeddings are a method of extracting features out of text so that we can input those features into a machine learning model to work with text data. They try to preserve syntactical and semantic information. The methods such as Bag of Words (BOW), CountVectorizer, and TF-IDF rely on the word count in a sentence but do not save any syntactical or semantic information. In these algorithms, the size of the vector is the number of elements in the vocabulary. We can get a sparse matrix if most of the elements are zero. Large input vectors will mean a huge number of weights which will result in the high computation required for training. Word Embeddings give a solution to these problems. There are two different approaches to get Word Embeddings.\n",
        "\n",
        "## 1.   **Word2Vec:**\n",
        "In Word2Vec, every word is assigned a vector. We start with either a random vector or a one-hot vector.\n",
        "\n",
        "**One-Hot Vector:** A representation where only one bit in a vector is 1. If there are 500 words in the corpus, then the vector length will be 500. After assigning vectors to each word, we take a window size and iterate through the entire corpus. While we do this, two neural embedding methods are used:\n",
        "\n",
        "#### 1.1.  **Continuous Bag of Words (CBOW):**\n",
        "In this model, we try to fit the neighboring words in the window to the central word.\n",
        "\n",
        "![image.png](https://media.geeksforgeeks.org/wp-content/uploads/cbow-1.png)\n",
        "\n",
        "#### 1.2.  **Skip Gram:**\n",
        "In this model, we try to make the central word closer to the neighboring words. It is the complete opposite of the CBOW model. It is shown that this method produces more meaningful embeddings.\n",
        "\n",
        "![image.png](https://media.geeksforgeeks.org/wp-content/uploads/skip_gram.png)\n",
        "\n",
        "After applying the above neural embedding methods, we get trained vectors of each word after many iterations through the corpus. These trained vectors preserve syntactical or semantic information and are converted to lower dimensions. The vectors with similar meaning or semantic information are placed close to each other in space.\n",
        "\n",
        "## 2.   **GloVe:**\n",
        "\n",
        "### **Common Errors made:**\n",
        "\n",
        "*   We need to use the exact same pipeline during deploying our model as was used to create the training data for the word embedding. If we use a different tokenizer or different method of handling white space, punctuation, etc. we might end up with incompatible inputs.\n",
        "\n",
        "*   Words in our input that do not have a pre-trained vector are known as Out of Vocabulary words ($OOV$). We should replace those words with $\"UNK\"$, which means unknown, and then handle them separately.\n",
        "\n",
        "*   **Dimension Mismatch:** Vectors can be of many lengths. If we train a model with vectors of length (say 400) and then try to apply vectors of length 1000 at inference time, we will run into errors. So make sure to use the same dimensions throughout.\n",
        "\n",
        "### **References:**\n",
        "\n",
        "*   [**Word Embeddings in NLP - GeeksforGeeks**](https://www.geeksforgeeks.org/word-embeddings-in-nlp/)\n",
        "\n",
        "*   [**Word Embeddings Blog**](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n",
        "\n",
        "*   [**The Illustrated Word2vec**](https://jalammar.github.io/illustrated-word2vec/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YP01vXwITXw"
      },
      "source": [
        "!pip install texthero\n",
        "!pip install textblob\n",
        "!pip install spacy==3.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moBsiw1B-QFj"
      },
      "source": [
        "# Import Library.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import texthero as hero\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Vocabulary Size.\n",
        "voc_size = 10000\n",
        "\n",
        "# Read Dataset.\n",
        "data = pd.read_csv(\"spam.csv\")\n",
        "\n",
        "# Text Cleaning and Preprocessing.\n",
        "data[\"Message\"] = data[\"Message\"].pipe(hero.clean).pipe(hero.remove_urls)\n",
        "data[\"Message\"] = data[\"Message\"].apply(\n",
        "    lambda x: str(TextBlob(x).correct())\n",
        ")  # Spelling Correction.\n",
        "data[\"Class\"] = data[\"Category\"].apply(lambda x: 1 if x == \"spam\" else 0)\n",
        "\n",
        "# Split Dataset into Dependent and Independent Features.\n",
        "X = data[\"Message\"]\n",
        "y = data[\"Class\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axMrudycIJI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ed7740-4e96-4141-a431-642624c187d2"
      },
      "source": [
        "# One Hot Representation.\n",
        "onehot_repr = [one_hot(words, voc_size) for words in X]\n",
        "\n",
        "max_length = 15\n",
        "embedded_docs = pad_sequences(onehot_repr, padding=\"post\", maxlen=max_length)\n",
        "print(embedded_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2965 3992 4165 ... 6374 5709 3435]\n",
            " [6852 3870 2317 ...    0    0    0]\n",
            " [5065 1102  883 ... 1985 9646  652]\n",
            " ...\n",
            " [ 344 6988 2796 ...    0    0    0]\n",
            " [5913 6513 9233 ... 5487    0    0]\n",
            " [5082 9877 4832 ...    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcih4LGzIAbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ca0936-abf9-4b97-f9b3-a8ab84eb2d72"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(voc_size, 10, input_length=max_length))\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 15, 10)            100000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 100,000\n",
            "Trainable params: 100,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWtpIqcCcaeU"
      },
      "source": [
        "print(model.predict(embedded_docs))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}