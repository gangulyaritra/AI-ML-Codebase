{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpN2PzFDM7e1"
      },
      "source": [
        "# **Natural Language Processing**\n",
        "\n",
        "Natural Language Processing (NLP) is a technique in artificial intelligence that deals with the understanding of human-based language. It involves programming techniques to create a model that can understand language, classify content, and even generate and create new compositions in human-based language.\n",
        "\n",
        "### **Reference**\n",
        "\n",
        "*   [**NLP Zero to Hero - YouTube Playlist**](https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S)\n",
        "\n",
        "## **Top NLP Libraries**\n",
        "\n",
        "*   Natural Language Toolkit (NLTK)\n",
        "*   Gensim\n",
        "*   Texthero\n",
        "*   spaCy\n",
        "*   TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3c5lMDgNkFs"
      },
      "source": [
        "### **Encode Language into Numbers.**\n",
        "\n",
        "Encoding language into numbers can be performed in many ways. The most common way is to encode entire words.\n",
        "\n",
        "Using this technique, consider a sentence like \"I love my dog\". We could encode that with the numbers $[1, 2, 3, 4]$. If we then wanted to encode another sentence like \"I love my cat\", it could be $[1, 2, 3, 5]$. The above two sentences have a similar meaning because they’re similar numerically, i.e., $[1, 2, 3, 4]$ looks a lot like $[1, 2, 3, 5]$. This process is called ***Tokenization***.\n",
        "\n",
        "*TensorFlow Keras contains a library called **preprocessing** that provides several extremely useful tools to prepare data for machine learning. One of these is a **Tokenizer** that will allow us to take words and turn them into tokens.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iisqKqLMmFt"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxg1B5EwQO81",
        "outputId": "a9e7bbc9-8e4a-42e3-d810-3e4a36701907"
      },
      "source": [
        "sentences = [\"I love my dog\", \"I love my cat\"]\n",
        "\n",
        "\"\"\"\n",
        "In this case, we create a Tokenizer object and specify the number of words that it can tokenize.\n",
        "This value will be the maximum number of tokens to generate from the corpus of words.\n",
        "We have a very small corpus here containing only five unique words, so we'll be well under the one hundred specified.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "\n",
        "# Once we have a tokenizer, calling \"fit_on_texts\" will create the tokenized word index.\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Print a set of key/value pairs for the words in the corpus.\n",
        "print(word_index)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQLdcm-ZTNws",
        "outputId": "d418c2c1-c3f6-4f62-e031-af4a20314945"
      },
      "source": [
        "\"\"\"\n",
        "The tokenizer is quite flexible. For example, if we were to expand the corpus with another sentence containing the word \"cat\"\n",
        "but with a question mark after it, the results show that it would be smart enough to filter out \"cat?\" as just \"cat\".\n",
        "\"\"\"\n",
        "\n",
        "sentences = [\"I love my dog\", \"I love my cat\", \"Do you love my cat?\"]\n",
        "\n",
        "\"\"\"\n",
        "This behavior is controlled by the filters parameter to the tokenizer, which defaults to removing all punctuation \n",
        "except the apostrophe character. Once we have the words in our sentences tokenized, the next step is to convert the \n",
        "sentences into lists of numbers, with the number being the value where the word is the key.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'cat': 4, 'dog': 5, 'do': 6, 'you': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y31WpVfU1Ke"
      },
      "source": [
        "### **Turning Sentences into Sequences:** *Encode the sentences into sequences of numbers.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Mi3AY2VI-K",
        "outputId": "7a84ca55-1751-438a-ac4e-5986c3fbd2bb"
      },
      "source": [
        "sentences = [\"I love my dog\", \"I love my cat\", \"Do you love my cat?\"]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "print(\"\\n\")\n",
        "\n",
        "# The tokenizer has a method called \"text_to_sequences\". Using it to our list of sentences, and will return a list of sequences.\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# The output is the sequences representing the three sentences.\n",
        "print(sequences)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'cat': 4, 'dog': 5, 'do': 6, 'you': 7}\n",
            "\n",
            "\n",
            "[[3, 1, 2, 5], [3, 1, 2, 4], [6, 7, 1, 2, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywznf8R_Z9gY"
      },
      "source": [
        "### **Using Out-Of-Vocabulary (OOV) tokens**\n",
        "\n",
        "Consider we are training a neural network on a set of data. The typical pattern is that we have a set of data used for training that we know won’t cover 100% of our needs, but we hope covers as much as possible. In the case of NLP, we might have many thousands of words in our training data, used in many different contexts, but we can’t have every possible word in every possible context. So when we show our neural network some new, previously unseen text containing previously unseen words, what might happen? The neural network will get confused because it simply has no context for those words, and, as a result, any prediction it gives will be negatively affected.\n",
        "\n",
        "One tool to use to handle these situations is an ***out-of-vocabulary (OOV)*** token. This method can help the neural network to understand the context of the data containing previously unseen text. For example, given the previous small example corpus, suppose we want to process sentences like these:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AApVI0pTZ8mh",
        "outputId": "1d5c83d0-79fa-4f97-9953-ebf763b27dec"
      },
      "source": [
        "test_data = [\"Your dog is beautiful.\", \"My cat ate your rat?\"]\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
        "print(word_index)\n",
        "print(test_sequences)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'cat': 4, 'dog': 5, 'do': 6, 'you': 7}\n",
            "[[5], [2, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0oXM0utHI39",
        "outputId": "49f2982c-6a00-459f-8bd7-6a89f2fd0906"
      },
      "source": [
        "# We do this by adding a parameter called \"oov_token\", as shown below.\n",
        "# We can assign it to any string we like, but make sure it does not appear elsewhere in our corpus.\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_sequences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'cat': 5, 'dog': 6, 'do': 7, 'you': 8}\n",
            "[[1, 6, 1, 1], [3, 5, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNQsxGuMKxXV"
      },
      "source": [
        "The output has improved a bit. Our tokens list has a new item, \"$<OOV>$\", and our test sentences maintain their length. The former is much closer to the original meaning. The latter, because most of its words aren't in the corpus, still lacks a lot of contexts, but it's a step in the right direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_eea9-eKyPT"
      },
      "source": [
        "### **Using Padding**\n",
        "\n",
        "When training neural networks, we typically need all our data to be in the same shape. That is, once we've tokenized the words and converted the sentences into sequences, they can all be in different lengths. To get them to be the same size and shape, we can use ***padding***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTgN-c7QUHmX",
        "outputId": "94ccfc5b-fb04-43cc-a2b3-3f92b3b04393"
      },
      "source": [
        "\"\"\" To explore padding, let's add another, much longer, sentence to the corpus. \"\"\"\n",
        "\n",
        "sentences = [\n",
        "    \"I love my dog\",\n",
        "    \"I love my cat\",\n",
        "    \"Do you love my cat?\",\n",
        "    \"The dog chased the cat and the cat chased the rat.\",\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\"\"\"\n",
        "The output is the sequences representing the four sentences. \n",
        "When we sequence that, we'll see that our lists of numbers have different lengths.\n",
        "\"\"\"\n",
        "print(sequences)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5, 3, 4, 6], [5, 3, 4, 1], [8, 9, 3, 4, 1], [2, 6, 7, 2, 1, 10, 2, 1, 7, 2, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIiLvVCCWCTu",
        "outputId": "21e9caa2-cf62-4b1f-e3b8-a49762e4d9b3"
      },
      "source": [
        "\"\"\" If we want to make these sequences into the same length, we can use the \"pad_sequences\" API. \"\"\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Using the \"pad_sequences\" API is very straightforward.\n",
        "# To convert our (unpadded) sequences into a padded set, we simply call \"pad_sequences\" like this:\n",
        "padded = pad_sequences(sequences)\n",
        "\n",
        "# We'll get a nicely formatted set of sequences. They'll also be on separate lines, like this:\n",
        "print(padded)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  5  3  4  6]\n",
            " [ 0  0  0  0  0  0  0  5  3  4  1]\n",
            " [ 0  0  0  0  0  0  8  9  3  4  1]\n",
            " [ 2  6  7  2  1 10  2  1  7  2 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxSaI8IoZNg7"
      },
      "source": [
        "The sequences get padded with 0, which isn't a token in our word list. If you had wondered why the token list began at 1 when typically programmers count from 0, now you know! \n",
        "\n",
        "We now have something that's regularly shaped that we can use for training. But before going there, let's explore this API a little because it gives us many options that we can use to improve our data. First, we might have noticed that in the case of the shorter sentences, to get them to be the same shape as the longest one, the requisite number of zeros was added at the beginning. This method is called ***pre-padding***, and it’s the default behavior. We can change this using the padding parameter. For example, if we want our sequences to be padded with zeros at the end, we can use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bijqAQoXYywM",
        "outputId": "7aaa2ecd-9e0a-42d6-827c-d8c3e05e74ce"
      },
      "source": [
        "padded = pad_sequences(sequences, padding=\"post\")\n",
        "\n",
        "# The words are at the beginning of the padded sequences, and the 0's characters are at the end.\n",
        "print(padded)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5  3  4  6  0  0  0  0  0  0  0]\n",
            " [ 5  3  4  1  0  0  0  0  0  0  0]\n",
            " [ 8  9  3  4  1  0  0  0  0  0  0]\n",
            " [ 2  6  7  2  1 10  2  1  7  2 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLl6yDN0c8lR"
      },
      "source": [
        "The next default behavior we may have observed is that the sentences were all made to be the same length as the longest one. It's a sensible default because it means we don’t lose any data. The trade-off is we get a lot of padding. But what if we don’t want this, perhaps because we have one crazy long sentence that means we would have too much padding in the padded sequences. To fix this, we can use the \"$maxlen$\" parameter, specifying the desired maximum length when calling \"$pad\\_sequences$\", like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvVS_DU2dAuq",
        "outputId": "f44c9428-aa5a-459f-ee0a-f23ed7b341ba"
      },
      "source": [
        "padded = pad_sequences(sequences, padding=\"post\", maxlen=6)\n",
        "print(padded)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5  3  4  6  0  0]\n",
            " [ 5  3  4  1  0  0]\n",
            " [ 8  9  3  4  1  0]\n",
            " [10  2  1  7  2 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-rXTPyZiim-"
      },
      "source": [
        "Now the padded sequences are all the same length, and there isn’t too much padding. We have lost some words from our longest sentence, though, and they’ve been truncated from the beginning. What if we don't want to lose the words from the beginning, but instead, want them truncated from the end of the sentence? We can override the default behavior with the truncating parameter, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5Mf88q5i4-n",
        "outputId": "e785fb40-4166-4ab4-a236-ba0ae76ec7cb"
      },
      "source": [
        "padded = pad_sequences(sequences, padding=\"post\", maxlen=6, truncating=\"post\")\n",
        "\n",
        "# The result will show that the longest sentence is now truncated at the end instead of the beginning.\n",
        "print(padded)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5  3  4  6  0  0]\n",
            " [ 5  3  4  1  0  0]\n",
            " [ 8  9  3  4  1  0]\n",
            " [ 2  6  7  2  1 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **spaCy: Industrial-strength NLP**\n",
        "\n",
        "> [**spaCy - Official Website**](https://spacy.io/)\n",
        "\n",
        "> [**spaCy - Wikipedia**](https://en.wikipedia.org/wiki/SpaCy)\n",
        "\n",
        "> [**spaCy - GitHub**](https://github.com/explosion/spaCy)\n",
        "\n",
        "> [**spaCy - PyPI**](https://pypi.org/project/spacy/)"
      ],
      "metadata": {
        "id": "3kItJNuEas-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "MYQDMQfiatOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Named Entity Recognition**\n",
        "\n",
        "**Named Entity Recognition** [**[Wikipedia]**](https://en.wikipedia.org/wiki/Named-entity_recognition) is the process of NLP that deals with identifying and classifying named entities. The raw and structured text gets parsed, and the named entities get classified into persons, organizations, places, money, time, etc. Named Entities are identified and segmented into various pre-defined classes. Named Entity Recognition (NER), also known as entity chunking/extraction, is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various pre-defined classes.\n",
        "\n",
        "NER systems are developed with various linguistic approaches, as well as statistical and machine learning methods. NER has many applications for project or business purposes. NER model first identifies an entity and then categorizes the entity into the most suitable class."
      ],
      "metadata": {
        "id": "weJeyvZ185IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Name Entity Recognition Function.\n",
        "def named_entity_recognition(raw_text):\n",
        "    NER = nlp(raw_text)\n",
        "    for word in NER.ents:\n",
        "        print(word.text, word.start_char, word.end_char, word.label_)\n",
        "    displacy.render(NER, style=\"ent\", jupyter=True)\n",
        "\n",
        "\n",
        "sentence = \"\"\" The Mars Orbiter Mission (MOM), informally known as Mangalyaan, was launched into Earth orbit on 5 November 2013 \n",
        "               by the Indian Space Research Organisation (ISRO) and has entered Mars orbit on 24 September 2014. India thus became \n",
        "               the first country to enter Mars orbit on its first attempt. It was completed at a record-low cost of $74 million. \"\"\"\n",
        "\n",
        "# Function Call.\n",
        "named_entity_recognition(sentence)"
      ],
      "metadata": {
        "id": "VOr2BGdVlUbM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "e356b60b-3c68-46ec-bc10-7d177dbc7593"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mars Orbiter Mission (MOM 1 30 PRODUCT\n",
            "Mangalyaan 53 63 PERSON\n",
            "Earth 83 88 LOC\n",
            "5 November 2013 98 113 DATE\n",
            "the Indian Space Research Organisation 133 171 ORG\n",
            "Mars 195 199 LOC\n",
            "24 September 2014 209 226 DATE\n",
            "India 228 233 GPE\n",
            "first 266 271 ORDINAL\n",
            "Mars 289 293 LOC\n",
            "$74 million 363 374 MONEY\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    The Mars Orbiter Mission (MOM\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              "), informally known as \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mangalyaan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", was launched into \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Earth\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " orbit on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    5 November 2013\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " </br>               by \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Indian Space Research Organisation\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (ISRO) and has entered \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mars\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " orbit on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    24 September 2014\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " thus became </br>               the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " country to enter \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mars\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " orbit on its first attempt. It was completed at a record-low cost of \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $74 million\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ". </div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parts-of-Speech Tagging**"
      ],
      "metadata": {
        "id": "ChcjAOaczTfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sentence = \"\"\"The Indian Space Research Organisation is the national space agency of India, headquartered in Bengaluru.\"\"\"\n",
        "\n",
        "# Part-of-Speech Tagging.\n",
        "doc = nlp(sentence)\n",
        "for token in doc:\n",
        "    print(token.text, \"|\", token.pos_, \"|\", token.tag_)"
      ],
      "metadata": {
        "id": "9YiS2WXvpU2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731652e3-7b6c-4286-a8e5-417bdc5154c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The | DET | DT\n",
            "Indian | PROPN | NNP\n",
            "Space | PROPN | NNP\n",
            "Research | PROPN | NNP\n",
            "Organisation | PROPN | NNP\n",
            "is | AUX | VBZ\n",
            "the | DET | DT\n",
            "national | PROPN | NNP\n",
            "space | PROPN | NNP\n",
            "agency | PROPN | NNP\n",
            "of | ADP | IN\n",
            "India | PROPN | NNP\n",
            ", | PUNCT | ,\n",
            "headquartered | VERB | VBN\n",
            "in | ADP | IN\n",
            "Bengaluru | PROPN | NNP\n",
            ". | PUNCT | .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Semantic Textual Similarity**"
      ],
      "metadata": {
        "id": "CEvzn6AOZPGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
        "doc2 = nlp(\"Fast food tastes very good.\")\n",
        "\n",
        "# Similarity between the two documents.\n",
        "print(doc1, \"<->\", doc2, \"<->\", doc1.similarity(doc2))\n",
        "\n",
        "# Similarity of tokens and spans.\n",
        "french_fries = doc1[2:4]\n",
        "burgers = doc1[5]\n",
        "print(french_fries, \"<->\", burgers, \"<->\", french_fries.similarity(burgers))"
      ],
      "metadata": {
        "id": "pMdQxIn6Zw9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de87922-af0a-4fab-f016-89670b5dde3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like salty fries and hamburgers. <-> Fast food tastes very good. <-> 0.27134929909014804\n",
            "salty fries <-> hamburgers <-> 0.40727245807647705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Measuring Text Similarity Using BERT.**"
      ],
      "metadata": {
        "id": "EEyRYw8sofEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"bert-base-nli-mean-tokens\")"
      ],
      "metadata": {
        "id": "D0D0UTbLnw_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Three years later, the coffin was still full of Jello.\",\n",
        "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
        "    \"The person box was packed with jelly many dozens of months later.\",\n",
        "    \"He found a leprechaun in his walnut shell.\",\n",
        "]\n",
        "\n",
        "# Sentence Embedding.\n",
        "sentence_embeddings = model.encode(texts)\n",
        "print(\"Shape of Embeddings is \", sentence_embeddings.shape)\n",
        "\n",
        "# Similarity of the remaining sentence w.r.t. the first sentence.\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])"
      ],
      "metadata": {
        "id": "R4LerftwmSgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b53f726-9979-415f-b92b-7ce14042e2ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Embeddings is  (4, 768)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.33088914, 0.7219258 , 0.5548363 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}